{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"-joZUrKBxZlp"},"source":["### Install Transformers"]},{"cell_type":"code","metadata":{"id":"jbf8I6C3xRpu"},"source":["!pip3 install --quiet transformers"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2xLjTD2tA68E"},"source":["import tensorflow as tf"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-R5wUDm20_q9"},"source":["### Data Processing"]},{"cell_type":"markdown","metadata":{"id":"SHhezTdp6ZNc"},"source":["Load data"]},{"cell_type":"code","metadata":{"id":"Kk4bGHTR1Ba2"},"source":["import pandas as pd"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive/')"],"metadata":{"id":"OfId1fF2zrB6"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bY2nkFUJ1JbG"},"source":["data = pd.read_csv(\"/content/drive/My Drive/NLP/Project/NLPMiniProject_data.csv\")\n","print(data.head())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"B_jvGnes1TQt"},"source":["df = data[['ID','comment','parent_comment','down','score','top','topic','label']]"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split"],"metadata":{"id":"Rf6G8DYq0cWz"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VFKnF30c2hHs"},"source":["X_train, X_test, y_train, y_test = train_test_split(\n","    df['comment']+df['parent_comment'],\n","    df['label'],\n","    test_size=0.2, \n","    random_state=42\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_train.shape, X_test.shape"],"metadata":{"id":"B12RDjnt0gVi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3VFZi7TZ6blJ"},"source":["Tokenize data using Bert Tokenizer"]},{"cell_type":"code","metadata":{"id":"FRKLhEYkyd8h"},"source":["!pip install sentencepiece"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IAV1wcHf3Mri"},"source":["from transformers import *"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xca4ytQQ2o_x"},"source":["#Get BertTokenizer\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ijbhoGq80WuR"},"source":["#tokenizer.vocab.items()"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sentences = (df['comment']+df['parent_comment']).values"],"metadata":{"id":"zEXDa0ex08cl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["labels = df.label.values"],"metadata":{"id":"MAjbEcrC1pMX"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zP4OJIP04q_K"},"source":["tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NmXy2EiijmU1"},"source":["sentences[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"y55BhKZW3GHf"},"source":["#Check tokenized text\n","print(tokenized_texts[0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oCyLM9jL5SE2"},"source":["#We will use only first 200 tokens to do classification (this value can be changed)\n","max_length = 200\n","tokenized_texts = [sent[:max_length] for sent in tokenized_texts]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4dM2OE85C2uU"},"source":["for i in range(len(tokenized_texts)):\n","    sent = tokenized_texts[i]\n","    sent = ['[CLS]'] + sent + ['[SEP]']\n","    tokenized_texts[i] = sent"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zP7qJPVYEZk0"},"source":["print(tokenized_texts[0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bgFWkwEf3_MY"},"source":["#Convert tokens into IDs\n","input_ids = [tokenizer.convert_tokens_to_ids(sent) for sent in tokenized_texts]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"P4Slg7b84Sd7"},"source":["print(input_ids[0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UU3oms37Tu-c"},"source":["tokenizer.vocab.items()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DKsOey_557B_"},"source":["#Pad our tokens which might be less than max_length size\n","input_ids = tf.keras.preprocessing.sequence.pad_sequences(input_ids, maxlen=max_length+2, truncating='post', padding='post')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G6eDBxNp4dw_"},"source":["input_ids.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2ZdKrRK_68CK"},"source":["Split data between training and test"]},{"cell_type":"code","metadata":{"id":"qb-rtcT-6NbK"},"source":["from sklearn.model_selection import train_test_split"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vc8O7AKC7CQ4"},"source":["#80% data will be used for training while 20% will be used for test\n","trainX, testX, trainY, testY = train_test_split(input_ids, labels, test_size=0.2, random_state=12345)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OtJhkg4f7XBA"},"source":["Create Attention masks : Attention masks are useful to ignore padding tokens. Mask value will be set to 0 for padding tokens and 1 for actual tokens. We will create mask both for training and test data"]},{"cell_type":"code","metadata":{"id":"LATbNgo87VwS"},"source":["# Create attention masks for training\n","train_attn_masks = []\n","\n","# Create a mask of 1s for each token followed by 0s for padding\n","for seq in trainX:\n","  seq_mask = [float(i>0) for i in seq]\n","  train_attn_masks.append(seq_mask)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"inz335am8AQq"},"source":["# Create attention masks for Test\n","test_attn_masks = []\n","\n","# Create a mask of 1s for each token followed by 0s for padding\n","for seq in testX:\n","  seq_mask = [float(i>0) for i in seq]\n","  test_attn_masks.append(seq_mask)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KiTQGqn_POku"},"source":["print(train_attn_masks[100])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Dcq2O3ZN8XRr"},"source":["At this point, we have the data ready"]},{"cell_type":"markdown","metadata":{"id":"RDkWa9cZ8cpQ"},"source":["### Build Model"]},{"cell_type":"code","metadata":{"id":"iNcElQY48InX"},"source":["#Load Pre-trained Bert Model with a Binary Classification layer at the top.\n","#Huggingface library provides TFBertForSequenceClassification for the same\n","model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8XqXIguX8oMa"},"source":["model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UUoCTeRa-JyO"},"source":["# Prepare training: Compile tf.keras model with optimizer, loss and learning rate schedule \n","optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"],"metadata":{"id":"DR_u9sZ1YmOM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n","model.compile(optimizer=optimizer, loss=loss, metrics=[metric])"],"metadata":{"id":"HB5D3zmMYvzE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RVwj1wHk8-jR"},"source":["### Train Model"]},{"cell_type":"code","metadata":{"id":"chuCqqJw_8cc"},"source":["import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LWtlDD6PAYNP"},"source":["train_x_data = {'input_ids': np.array(trainX), 'attention_mask': np.array(train_attn_masks)}\n","test_x_data = {'input_ids': np.array(testX), 'attention_mask': np.array(test_attn_masks)}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ya5DYBlD9AUx"},"source":["model.fit(train_x_data, trainY, \n","          validation_data=(test_x_data, testY), \n","          batch_size=16, \n","          epochs=2)"],"execution_count":null,"outputs":[]}]}